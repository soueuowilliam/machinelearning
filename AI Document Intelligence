#LANGCHAIN IMPORTS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_classic.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

#SYSTEM IMPORTS
from dotenv import load_dotenv
import os

load_dotenv()
key = os.getenv('GOOGLE_API_KEY')

#EXTRAINDO OS DADOS
caminho = r'...'
pdf = DirectoryLoader(caminho, glob = '**/*pdf', loader_cls = PyPDFLoader).load()
doc = DirectoryLoader(caminho, glob = '**/*docx', loader_cls = PyPDFLoader).load()
dados = pdf + doc

#CHUNCKS
embeddings = GoogleGenerativeAIEmbeddings(model = 'text-embedding-004')
chunks = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 100).split_documents(dados)

#BANCO DE DADOS VETORIAL
banco_vetorial = FAISS.from_documents(chunks, embeddings).as_retriever(search_kwargs = {'k': 4})

#MODELO DE LLM
modelo = ChatGoogleGenerativeAI(model = 'gemini-2.5-flash', temperature = 0.5, google_api_key = key)

#HISTÃ“RICO DE CONVERSA
memoria = ConversationBufferMemory(k = 4, memory_key = 'historico', input_key = 'pergunta', return_messages = True)

#DEFININDO O QUE A RAG IRÃ FAZER
prompt = ChatPromptTemplate.from_messages(
    [
        ("system",
        "VocÃª Ã© um assistente de RAG. "
        "Responda APENAS com base no contexto fornecido. "
        "SEMPRE informe o nome do documento e a pÃ¡gina"
        "HistÃ³rico da conversa: \n{historico}"
        "Sempre pergunte se ele precisa de mais alguma informaÃ§Ã£o"
        "Se a resposta nÃ£o estiver no contexto, diga claramente que nÃ£o encontrou.")
        ,
        ('human',
        'Pergunta: {pergunta}\n\n'
        'Contexto: {contexto}\n\n'
        'Resposta:')
    ]
)


chain = prompt | modelo | StrOutputParser()

def responder(pergunta: str):
    documento = banco_vetorial.invoke(pergunta)
    
    #COMANDO PARA REALIZAR A LIMPEZA DA MEMÃ“RIA
    if pergunta.lower() in ['limpar', 'reset', 'exit', 'quit', 'clear']:
        memoria.clear()
        return "ğŸ§¹ MemÃ³ria limpa. Pode comeÃ§ar uma nova conversa."

    if not documento:
        return 'NÃ£o encontrei informaÃ§Ãµes relevantes no documento.'
    

    contexto = "\n\n".join(
        f"[Arquivo: {doc.metadata.get('source', 'N/A')} | PÃ¡gina {doc.metadata.get('page', 'N/A')}]\n"
        f"{doc.page_content}" for doc in documento)
    
   
    history = memoria.load_memory_variables({})['historico']  #CARREGANDO O HISTÃ“RICO NA MEMÃ“RIA
    resposta = chain.invoke({'pergunta': pergunta, 'contexto': contexto, 'historico': history})
    memoria.save_context({'pergunta':pergunta}, {'resposta': resposta}) #SALVANDO O HISTÃ“RICO NA MEMÃ“RIA
    
    fontes = "\n".join(f"{doc.metadata.get('source', 'N/A')} - pÃ¡gina {doc.metadata.get('page', 0) + 1}" for doc in documento)

    return f""" 
    RESPOSTA:
    {resposta}

    FONTES:
    {fontes}
    """


while True:
    resposta = responder(input('>> Pergunta: '))
    print(resposta)
