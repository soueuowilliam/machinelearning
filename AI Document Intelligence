#LANGCHAIN IMPORTS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, Docx2txtLoader
from langchain_classic.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

#SYSTEM IMPORTS
from dotenv import load_dotenv
import os

load_dotenv()
key = os.getenv('GOOGLE_API_KEY')
os.environ.get('LANGSMITH_API_KEY')

#EXTRAINDO OS DADOS
caminho = r'...' #CAMINHO LOCAL
pdf = DirectoryLoader(caminho, glob = '**/*pdf', loader_cls = PyPDFLoader).load()
word = DirectoryLoader(caminho, glob = '**/*docx', loader_cls= Docx2txtLoader).load()
dados = pdf + word

#CHUNCKS
embeddings = GoogleGenerativeAIEmbeddings(model = 'text-embedding-004')
chunk_semantic = SemanticChunker(embeddings, breakpoint_threshold_type = "percentile", breakpoint_threshold_amount = 95)
chunk = chunk_semantic.split_documents(dados)

#BANCO DE DADOS VETORIAL
banco_vetorial = FAISS.from_documents(chunk, embeddings).as_retriever(search_kwargs = {'k': 4})

#MODELO DE LLM
modelo = ChatGoogleGenerativeAI(model = 'gemini-2.5-flash', temperature = 0.5, google_api_key = key)

#HISTÃ“RICO DE CONVERSA
memoria = ConversationBufferMemory(memory_key = 'historico', input_key = 'pergunta')

#DEFININDO O QUE A RAG IRÃ FAZER
prompt = ChatPromptTemplate.from_messages(
    [
        ("system",
        """
        VocÃª Ã© um assistente de RAG. 
        
        REGRAS:
        -SEMPRE informe o nome do documento e a pÃ¡gina
        -NÃ£o informe o caminho do documento.
        -Caso eu peÃ§a, faÃ§a uma anÃ¡lise dos documentos.
        -NÃ£o invente informaÃ§Ãµes, passe apenas o que estÃ¡ no documento.
        -Sempre pergunte se ele precisa de mais alguma informaÃ§Ã£o
        -Se a resposta nÃ£o estiver no contexto, diga claramente que nÃ£o encontrou.
        
        HistÃ³rico da conversa:
        {historico}
        """)
        ,
        ('human',
        """
        Contexto:
        {contexto}

        Pergunta:
        {pergunta}

        Resposta:
        """)
    ]
)

def formatar_contexto(documentos):
    textos = []

    for doc in documentos:
        arquivo = os.path.basename(doc.metadata.get("source", "N/A"))
        pagina = doc.metadata.get("page")

        pagina_txt = (
            f"PÃ¡gina {pagina + 1}"
            if pagina is not None
            else "PÃ¡gina nÃ£o aplicÃ¡vel"
        )

        textos.append(
            f" >> Arquivo: {arquivo} | {pagina_txt}\n{doc.page_content}"
        )

    return "\n\n".join(textos)


def carregar_historico(_):
    return memoria.load_memory_variables({}).get('historico', '')

chain = {
    'contexto' : banco_vetorial | formatar_contexto,
    'pergunta' : RunnablePassthrough(),
    'historico': carregar_historico } | prompt | modelo | StrOutputParser()

def responder(pergunta: str):

    #COMANDO PARA REALIZAR A LIMPEZA DA MEMÃ“RIA
    if pergunta.lower() in ['limpar', 'reset', 'clear']:
        memoria.clear()
        return "ðŸ§¹ MemÃ³ria limpa. Pode comeÃ§ar uma nova conversa."
    
    resposta = chain.invoke(pergunta)

    memoria.save_context({'pergunta':pergunta}, {'resposta': resposta}) #SALVANDO O HISTÃ“RICO NA MEMÃ“RIA

    return f'\nRESPOSTA:\n{resposta}\n'

if __name__ == "__main__":
    print("ðŸ¤– RAG iniciado. Digite sua pergunta ('limpar' ou 'sair'):")

    while True:
        pergunta_usuario = input("\n>> Pergunta: ")
        if pergunta_usuario.lower() in ['sair', 'quit']:
            break

        print(responder(pergunta_usuario))

#Como devo proceder caso tenha um item roubado?
